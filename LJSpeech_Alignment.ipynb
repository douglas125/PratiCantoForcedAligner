{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d98e1de-45ae-406b-882a-6394340de14e",
   "metadata": {},
   "source": [
    "# Alignment on LJSpeech Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccea284-4539-455b-9159-8827287699a8",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e5582-4d9d-488d-a5bd-04a30f1a6a23",
   "metadata": {},
   "source": [
    "!du -sh ../..\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45cfd8-0016-4cb1-be19-4dceea4442bd",
   "metadata": {},
   "source": [
    "!ls ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9466b-2b05-4f7a-ba57-51760ca1c759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb231c-234e-43c8-888c-49dd6eddbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas -q\n",
    "%pip install tqdm -q\n",
    "%pip install ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffac321-f109-426f-ab28-db03d5e51a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_readers.ljspeech_reader import LJSpeechReader  # noqa\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "gpu_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2fffb-414f-41f8-8e04-d5179cdd105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False\n",
    "if using_colab:\n",
    "    !wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "    !tar -xf LJSpeech-1.1.tar.bz2 --checkpoint=.5000\n",
    "    !rm LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409b005-d526-4a59-8fc6-3cfe9d12f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljs_file = r'LJSpeech-1.1'\n",
    "ljsr = LJSpeechReader(ljs_file)\n",
    "gen = ljsr.generate_audios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455070c9-5631-4852-b5d7-e213fba01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_id, cur_txt, cur_audio, sr = next(gen)\n",
    "s = LJSpeechReader.serialize(cur_id, cur_txt, cur_audio, sr)\n",
    "cur_id, cur_txt, cur_audio, sr = LJSpeechReader.deserialize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474d3ce-a0d2-473c-9658-5eb8b4d8cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cur_id, cur_txt)\n",
    "ipd.Audio(cur_audio[:, 0].numpy(), rate=sr.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45d442-a042-43d9-9546-d34572b60e46",
   "metadata": {},
   "source": [
    "# Write tfrecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c0d2e-82e7-4e6e-8634-b2ded32f4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljstfrecords = 'ljspeech.tfrecords'\n",
    "if not os.path.isfile(ljstfrecords):\n",
    "    ljsr.write_tfrecords_file(ljstfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d34f1-d4d6-4db8-801c-26e7d43799b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(\n",
    "    'ljspeech.tfrecords'\n",
    ").map(LJSpeechReader.deserialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bb4f1-a9c7-47cf-9d5e-22ec115bc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [x for x in dataset.skip(5).take(1)][0]\n",
    "cur_id, cur_txt, cur_audio, sr = sample[0], sample[1], sample[2], sample[3]\n",
    "\n",
    "print(cur_id, cur_txt, sr.numpy())\n",
    "ipd.Audio(cur_audio[:, 0].numpy(), rate=sr.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7312545-07d9-454c-8132-6ec521fef0a0",
   "metadata": {},
   "source": [
    "## Inspect models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d894801-b662-4d57-b6ab-8fab7a1b38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.alignment_model import PraticantoForcedAligner  # noqa\n",
    "from models import alignment_losses  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25506ed2-57d9-4f39-a88c-3f095ed77d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfa = PraticantoForcedAligner(vocab=ljsr.tokens, sampling_rate=22050)\n",
    "alignment_model = pfa.build_models()\n",
    "alignment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ba099-84ef-4093-acd8-22dd37cc65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(cur_txt, axis=0).shape, tf.expand_dims(cur_audio[:, 0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6c231-74e3-479e-b193-3175091c5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_model([\n",
    "    tf.expand_dims(tf.strings.bytes_split(cur_txt), axis=0),\n",
    "    tf.expand_dims(cur_audio[:, 0], axis=0)\n",
    "]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0ae0c-416a-4449-a6b2-9a5a968bbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'This is my text and it is quite long'\n",
    "char_input = tf.expand_dims(tf.strings.bytes_split(sample), axis=0)\n",
    "audio = tf.zeros((1, sr * 2))\n",
    "out = alignment_model([char_input, audio])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1286e-ee40-4930-a807-b38234586699",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.convert_to_tensor([\n",
    "    [\n",
    "        [1, 1, 1,   0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0,   1, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 0, 0, 0],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1,   0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0,   1, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 1, 1, 0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 0, 0, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 0.2, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0.8, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 1, 1, 0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 0, 0, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1,   0, 0, 0, 0, 0, 0, 0.0],\n",
    "        [0, 0, 0,   1, 1, 0, 0, 0, 0, 0.0],\n",
    "        [0, 0, 0,   0, 0, 1, 1, 0, 1, 0.0],\n",
    "        [0, 0, 0,   0, 0, 0, 0, 1, 0, 0.1],\n",
    "        [1, 1, 0,   0, 1, 0, 0, 0, 0, 0.1],\n",
    "    ],\n",
    "])\n",
    "unpadded_shapes = [\n",
    "    [4, 10],\n",
    "    [5, 10],\n",
    "    [5, 10],\n",
    "    [4, 8],\n",
    "]\n",
    "a_loss = alignment_losses.alignment_loss()\n",
    "a_loss(unpadded_shapes, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4fb1b-1dfd-4f79-ae1c-76b8cfb38915",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9e056-a743-46b6-a623-d55c37334ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(cur_id, cur_txt, cur_audio, sr):\n",
    "    cur_txt = tf.ensure_shape(cur_txt, ())\n",
    "    cur_txt = tf.strings.bytes_split(cur_txt)\n",
    "    # cur_txt = tf.concat([['[BOS]'], cur_txt, ['[EOS]']], axis=0)\n",
    "\n",
    "    shapes = tf.concat([\n",
    "        tf.shape(cur_txt),\n",
    "        1 + (tf.shape(cur_audio[:, 0]) - pfa.frame_length) // pfa.frame_step\n",
    "\n",
    "        # tf.cast(tf.math.ceil(\n",
    "        #     (tf.shape(cur_audio[:, 0]) - pfa.frame_length) // pfa.frame_step\n",
    "        # ) + 1, tf.int32)\n",
    "\n",
    "    ], axis=0)\n",
    "    return cur_txt, cur_audio[:, 0], shapes\n",
    "\n",
    "\n",
    "def prep_batch_inputs(cur_txt, cur_audio, seq_lengths):\n",
    "    return {\n",
    "        'char_seq': cur_txt,\n",
    "        'waveform': cur_audio,\n",
    "    }, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac13a4d-b244-4db1-afcc-f3a560858044",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = pfa.char_table('[PAD]')\n",
    "pad_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f03006-e80b-47c4-9e70-911770d1a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.TFRecordDataset(\n",
    "    'ljspeech.tfrecords'\n",
    ").shuffle(6 * batch_size).repeat().map(LJSpeechReader.deserialize).map(\n",
    "    prep_inputs\n",
    ").padded_batch(\n",
    "    # batch_size, padding_values=(pad_index, 0.0), padded_shapes=(200, 400000)\n",
    "    batch_size, padding_values=('[PAD]', 0.0, 0), drop_remainder=True\n",
    ").map(prep_batch_inputs).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f782e77-d2c1-4081-b68a-06097ac829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [x for x in dataset.take(1)]\n",
    "sample[0][0]['char_seq'].shape, sample[0][0]['waveform'].shape, str(sample[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f5703-8a42-46a8-8e17-22cc6a402d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.join(sample[0][0]['char_seq'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14848f-c973-4341-92eb-1c02bf737b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfa.MelSpectrogram(tf.zeros((219293,))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2398c4d-ed52-4369-8e7d-596f2204b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1023 * 10 - 1024) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ead1f-8930-4262-a614-0f218e84ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(unpadded_shape, padded_shape):\n",
    "    \"\"\" Creates a mask that is 1 in unpadded shape and zero elsewhere\n",
    "    e.g.\n",
    "    1 1 1 1 1 1 0 0 0 0\n",
    "    1 1 1 1 1 1 0 0 0 0\n",
    "    1 1 1 1 1 1 0 0 0 0\n",
    "    1 1 1 1 1 1 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    \"\"\"\n",
    "    vec = tf.ones(unpadded_shape)\n",
    "    pad_shape = tf.stack([\n",
    "        tf.zeros((2,), dtype=tf.int32),\n",
    "        padded_shape - unpadded_shape\n",
    "    ], axis=1)\n",
    "    vec = tf.pad(vec, pad_shape)\n",
    "    return vec\n",
    "\n",
    "\n",
    "s1 = tf.convert_to_tensor([72, 391])\n",
    "s2 = tf.convert_to_tensor([166, 792])\n",
    "v1 = tf.ones(s1)\n",
    "\n",
    "pad_shape = tf.stack([\n",
    "    tf.zeros((2,), dtype=tf.int32),\n",
    "    s2-s1\n",
    "], axis=1)\n",
    "\n",
    "v2 = tf.pad(v1, pad_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8443ec-f17a-465b-aa9a-9a2d7c135258",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2af54d-fbc4-40c5-a5f1-18fa7634f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([\n",
    "    tf.zeros((2,), dtype=tf.int32),\n",
    "    s2-s1\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc83c37-d587-4d29-a185-d10b1cd93ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls checkpoints -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6822ef-22b7-47ae-864c-116280eaa6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp checkpoints/m_13_0.414.chkpt*.* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43d753-d837-46ad-a17e-bcb7d5815265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_losses = [\n",
    "    alignment_losses.alignment_loss(x)\n",
    "    for x in alignment_losses.possible_losses\n",
    "]\n",
    "print(model_losses)\n",
    "\n",
    "alignment_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3, clipnorm=0.1, beta_1=0.8, beta_2=0.99, epsilon=0.1),\n",
    "    loss=model_losses[0],\n",
    "    metrics=model_losses[1:],\n",
    ")\n",
    "alignment_model.load_weights('checkpoints/m_44_0.397.chkpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c971e-7986-4d12-9d8a-c91471f52157",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf checkpoints\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "filepath = 'checkpoints/m_{epoch}_{loss:.3f}.chkpt'\n",
    "chkpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch',\n",
    ")\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 1e-4\n",
    "    if epoch <= 1:\n",
    "        return 2e-6\n",
    "    elif epoch == 2:\n",
    "        return lr * 10\n",
    "    elif epoch == 4:\n",
    "        return lr * 10\n",
    "    else:\n",
    "        return lr\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.2, patience=10, verbose=1,\n",
    "    mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e6cc9-4c66-445c-9eb2-69e7cca68285",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_model.fit(\n",
    "    dataset,\n",
    "    epochs=300,\n",
    "    steps_per_epoch=len(ljsr.df_audios) // batch_size,\n",
    "    callbacks=[lr_callback, chkpt_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafcfb9-8f5f-4dd5-9ee8-567fe57d6830",
   "metadata": {},
   "source": [
    "# Visual evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06befea6-bd59-41c7-a9e7-57852f3d232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "m_spec = models.alignment_model.get_spectrogram_model()\n",
    "m_logmel = models.alignment_model.get_melspec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d104d-2a6e-488a-92bc-08f2535c6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment_model.load_weights('checkpoints/m_1_33.717620849609375.chkpt')\n",
    "# samples[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b990a-a133-413e-9c6f-ef5cecbaf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [x for x in dataset.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5c157-1cf8-4d7a-a7a7-e148b397089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = alignment_model(samples[0][0])\n",
    "padded_char_len = preds.shape[1]\n",
    "preds.shape, samples[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935b90f-9423-473d-8ff7-5aade1802688",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "unpadded_lens = samples[0][1][idx]\n",
    "char_len = unpadded_lens[0].numpy()\n",
    "spec_len = unpadded_lens[1].numpy()\n",
    "print('Unpadded:', unpadded_lens)\n",
    "\n",
    "xmax = spec_len\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "# for k in range(0, padded_char_len):\n",
    "# for k in range(0, 15):\n",
    "# for k in [0, 1, 2, 3, 4, 5, 6, 7, 8, -2, -1]:\n",
    "for k in range(0, char_len, 1):\n",
    "    plt.plot(preds[idx, k, 0:spec_len].numpy(), label=str(k))\n",
    "    # plt.plot(preds[idx, k, :].numpy())\n",
    "    plt.ylim(0, 1)\n",
    "    # plt.show()\n",
    "# plt.legend()\n",
    "plt.xlim(0, xmax)\n",
    "\n",
    "\n",
    "audio_data = samples[0][0]['waveform'][idx]\n",
    "txt_data = tf.strings.join(samples[0][0]['char_seq'][idx]).numpy().decode('UTF-8').replace('[PAD]', '')\n",
    "\n",
    "logmel = m_logmel(tf.expand_dims(\n",
    "    audio_data, axis=0)\n",
    ")\n",
    "print(logmel.shape, txt_data)\n",
    "# t = tf.cast(tf.range(0, logmel.shape[1]), tf.float32) * 256.0 / tf.cast(sr, tf.float32)\n",
    "# mels = tf.range(0, logmel.shape[2], delta=1)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.pcolormesh(\n",
    "    # t.numpy(),\n",
    "    # mels.numpy(),\n",
    "    tf.transpose(logmel[0]).numpy()\n",
    ")\n",
    "plt.xlim(0, xmax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecf793-86d9-48a1-a8f2-1d83d847164a",
   "metadata": {},
   "source": [
    "## Decode prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e7337-344b-4d43-84ba-46763c8a4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.decoder import PFADecoder\n",
    "pfa_dec = PFADecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2a780-89eb-4114-bff0-2d069288f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = preds[idx, 0:char_len, 0:spec_len].numpy()\n",
    "print(m.shape[0] * m.shape[1])\n",
    "alignment = np.array(pfa_dec.decode_alignment(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0579cd-ce86-4001-9d4b-f2e90ea18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(alignment[:, 1], alignment[:, 0])\n",
    "plt.xlim(0, xmax)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.pcolormesh(\n",
    "    tf.transpose(logmel[0]).numpy()\n",
    ")\n",
    "plt.xlim(0, xmax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc03d1-b9f7-494e-bf45-cafa0dbbc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = tf.audio.encode_wav(tf.expand_dims(audio_data, 1), sr)\n",
    "tf.io.write_file('outputs/out.wav', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde607cf-75aa-4894-bb78-baaf5f8d6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_srt(chars, alignment, time_delta, filename):\n",
    "    char_dict = _compute_chardict(chars, alignment, time_delta, filename)\n",
    "    _write_chardict(char_dict, filename)\n",
    "\n",
    "\n",
    "def _write_chardict(char_dict, filename):\n",
    "    cur_annot = 1\n",
    "    with open(filename, 'w') as f:\n",
    "        use_sep = False\n",
    "        for x in char_dict:\n",
    "            xmin = char_dict[x]['t0']\n",
    "            xmax = char_dict[x]['tf']\n",
    "            txt = char_dict[x]['char']\n",
    "            if use_sep:\n",
    "                f.write('\\n\\n')\n",
    "            use_sep = True\n",
    "\n",
    "            f.write(f'{cur_annot}\\n')\n",
    "            # don't show in UI\n",
    "            base_txt = txt + '|||8760|||9760|||Arial, 44pt|||False|||'\n",
    "            xmin = convert_seconds_to_srt(xmin)\n",
    "            xmax = convert_seconds_to_srt(xmax)\n",
    "            f.write(f'{xmin} --> {xmax}\\n')\n",
    "            f.write(f'{base_txt}')\n",
    "            cur_annot += 1\n",
    "\n",
    "\n",
    "def _compute_chardict(chars, alignment, time_delta, filename):\n",
    "    char_dict = {}\n",
    "    char_dict[0] = {'char': chars[0], 't0': 0}\n",
    "    prev_char = 0\n",
    "    for char_idx, spec_idx in alignment:\n",
    "        if char_idx > prev_char:\n",
    "            char_dict[prev_char]['tf'] = spec_idx * time_delta\n",
    "            char_dict[char_idx] = {\n",
    "                'char': chars[char_idx], 't0': spec_idx * time_delta\n",
    "            }\n",
    "        prev_char = char_idx\n",
    "    char_dict[prev_char]['tf'] = alignment[-1][1] * time_delta\n",
    "    return char_dict\n",
    "\n",
    "\n",
    "def convert_seconds_to_srt(time_in_s):\n",
    "    # 00:00:01,417 --> 00:00:01,924\n",
    "    hours = int(time_in_s) // 3600\n",
    "    remaining = int(time_in_s) - 3600 * hours\n",
    "    minutes = remaining // 60\n",
    "    seconds = remaining - 60 * minutes\n",
    "\n",
    "    milliseconds = str(int(np.round(1000 * (time_in_s - int(time_in_s)))))\n",
    "\n",
    "    hours = str(hours).rjust(2, '0')\n",
    "    minutes = str(minutes).rjust(2, '0')\n",
    "    seconds = str(seconds).rjust(2, '0')\n",
    "    milliseconds = milliseconds.rjust(3, '0')\n",
    "    return f'{hours}:{minutes}:{seconds},{milliseconds}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b60a8f-9e7a-4b42-9280-1d458ad6d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'outputs/out.srt'\n",
    "time_delta = 256 / sr.numpy()\n",
    "write_srt(txt_data, alignment, time_delta, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe5226-3c91-482a-b0d7-f3c4004bac09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0be5b-d825-42ab-9420-4df5bbdf591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds_to_srt(time_in_s):\n",
    "    # 00:00:01,417 --> 00:00:01,924\n",
    "    hours = int(time_in_s) // 3600\n",
    "    remaining = int(time_in_s) - 3600 * hours\n",
    "    minutes = remaining // 60\n",
    "    seconds = remaining - 60 * minutes\n",
    "\n",
    "    milliseconds = str(int(np.round(1000 * (time_in_s - int(time_in_s)))))\n",
    "\n",
    "    hours = str(hours).rjust(2, '0')\n",
    "    minutes = str(minutes).rjust(2, '0')\n",
    "    seconds = str(seconds).rjust(2, '0')\n",
    "    milliseconds = milliseconds.rjust(3, '0')\n",
    "    return f'{hours}:{minutes}:{seconds},{milliseconds}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76757e0-6d44-4d6f-ad22-b22042eea75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_seconds_to_srt(1*3600 + 28*60 + 4.281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e808e-8359-402c-a61e-125d53fb4d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780026a-a386-4117-bb40-ecf5792f621c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82468641-1989-49df-8cfd-35249d6fb13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "009c95d5-099e-4dca-a7de-1122690b9747",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa02d7-2710-4cee-9936-190cfdaa4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = samples[0][0]['waveform'][idx]\n",
    "txt_data = tf.strings.join(samples[0][0]['char_seq'][idx]).numpy().decode('UTF-8').replace('[PAD]', '')\n",
    "\n",
    "logmel = m_logmel(tf.expand_dims(\n",
    "    audio_data, axis=0)\n",
    ")\n",
    "print(logmel.shape, txt_data)\n",
    "# t = tf.cast(tf.range(0, logmel.shape[1]), tf.float32) * 256.0 / tf.cast(sr, tf.float32)\n",
    "# mels = tf.range(0, logmel.shape[2], delta=1)\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.pcolormesh(\n",
    "    # t.numpy(),\n",
    "    # mels.numpy(),\n",
    "    tf.transpose(logmel[0]).numpy()\n",
    ")\n",
    "plt.xlim(0, spec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca6b43-8c8b-48ce-a749-a3c9d6620cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1f78d-a941-4014-968a-97b7a74c2f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57bd39-6274-4403-b205-cc14e041c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds[idx, 0, 0:spec_len].numpy())\n",
    "plt.plot(preds[idx, char_len - 1, 0:spec_len].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33f92e-b09d-4ab1-8d12-4700e142c7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5da48d-ad36-4906-8aac-73e79a31e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(preds[idx], axis=0), tf.reduce_max(preds[idx], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c86d9-2ddf-4cb2-a573-3489f7dcdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = m_spec(tf.expand_dims(\n",
    "    audio_data[0:16000 * 5, 0], axis=0)\n",
    ")\n",
    "print(spec.shape)\n",
    "t = tf.cast(tf.range(0, spec.shape[1]), tf.float32) * 256.0 / tf.cast(sr, tf.float32)\n",
    "freqs = tf.range(0, spec.shape[2], delta=1, dtype=tf.float32) * tf.cast(sr, tf.float32) / 1024.\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.pcolormesh(\n",
    "    t.numpy(),\n",
    "    freqs.numpy(),\n",
    "    tf.math.log(1e-6 + tf.transpose(spec[0])).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabab6e8-1248-4fe9-8e4f-18bd4a1092ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmel = m_logmel(tf.expand_dims(\n",
    "    audio_data[0:16000 * 5, 0], axis=0)\n",
    ")\n",
    "print(logmel.shape)\n",
    "t = tf.cast(tf.range(0, logmel.shape[1]), tf.float32) * 256.0 / tf.cast(sr, tf.float32)\n",
    "mels = tf.range(0, logmel.shape[2], delta=1)\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.pcolormesh(\n",
    "    t.numpy(),\n",
    "    mels.numpy(),\n",
    "    tf.transpose(logmel[0]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
